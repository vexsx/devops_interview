
---

# üìÅ `snapp_box_senior_devops_interview-2025_may`

## Kubernetes

---

### 1. **What is Kubernetes?**

Kubernetes is an open-source **container orchestration platform** designed to automate deployment, scaling, and operation of application containers across clusters of hosts.

> üìå You use Kubernetes when you need **automation**, **scalability**, **high availability**, and **self-healing** for containerized applications.

It shines in environments where:

* You need to run microservices at scale.
* You want to decouple workloads from infrastructure.
* You‚Äôre moving toward GitOps or declarative infrastructure.

However, Kubernetes isn‚Äôt always the best choice:

* Small workloads may be overkill.
* It introduces operational complexity.
* It requires a learning curve and infrastructure investment.

> **TL;DR**: Kubernetes is ideal when your system's **scale, complexity, or uptime** requirements justify its operational cost.

---

### 2. **What are the Core Components of Kubernetes?**

| Component                 | Description                                                                              |
| ------------------------- | ---------------------------------------------------------------------------------------- |
| `etcd`                    | A consistent and highly available **key-value store** for all cluster data.              |
| `kube-apiserver`          | The **entry point** for all API calls. Manages communication inside the cluster.         |
| `kube-scheduler`          | Decides **which node** a Pod should run on, based on resource requirements and policies. |
| `kube-controller-manager` | Manages **controllers** that regulate the cluster state (e.g., node, replication, job).  |
| `kubelet`                 | An agent that runs on each node to ensure containers are running in a Pod.               |
| `kube-proxy`              | Manages **network routing** for services on each node using iptables or IPVS.            |
| `Container Runtime`       | Executes and manages containers (e.g., containerd, CRI-O, Docker ‚Äì deprecated).          |

> All these components work together to provide a **resilient, self-healing, and declarative infrastructure** for containers.

---

### 3. **Is `kubelet` started on all nodes?**

Yes ‚Äî **`kubelet` must run on every node** (including control plane nodes if they're also running workloads, but it is not best practice).

> üß† The `kubelet` is the **primary node agent** that connects each node to the Kubernetes control plane.

#### üîß Why is it required?

* It **registers the node** with the cluster via the API server.
* It **monitors Pod specs** assigned to that node.
* It ensures that containers are **running and healthy**.
* It **communicates with the container runtime** (e.g., containerd) to manage containers.

> If a node doesn‚Äôt have `kubelet` running, it **cannot join** the cluster or run workloads.

---

### 4. **Who checks the probes of the Pod?**

The **`kubelet`** is responsible for checking both **liveness** and **readiness probes** of Pods running on its node.

#### üîç Why `kubelet`?

Because:

* `kubelet` manages the **lifecycle** of all Pods on the node.
* It watches the PodSpec (via the API server) and performs health checks according to the probe configuration.
* It uses HTTP, TCP, or command execution to run the probe logic defined in the Pod YAML.

#### üîÑ What happens when a probe fails?

* **Liveness probe fails**: `kubelet` kills the container ‚Üí it will be restarted by the controller (e.g., Deployment).
* **Readiness probe fails**: `kubelet` marks the Pod as **not ready** ‚Üí it is removed from Service load balancing.

---

### 5. **What is the Kubernetes Control Plane?**

The **control plane** is the **brain** of the Kubernetes cluster.

| Component                               | What It Does                                                            |
| --------------------------------------- | ----------------------------------------------------------------------- |
| `kube-apiserver`                        | The **front door**. All communication goes through here.                |
| `etcd`                                  | The **database** that stores everything (cluster state).                |
| `kube-scheduler`                        | Decides **which node** will run a new Pod.                              |
| `controller-manager`                    | Keeps the cluster in the **desired state** (e.g., scaling, restarting). |
| (Optionally) `cloud-controller-manager` | Talks to the cloud provider (e.g., creates load balancers).             |

> All of these run on **control plane nodes** (also called **master nodes**).

---

Great intuition again ‚Äî you're right that the `kube-apiserver` is the central communication point. Let‚Äôs rephrase and expand your answer into a clear and professional response, suitable for a senior DevOps interview:

---

### 6. **How does the `kubelet` communicate with the controller?**

The **`kubelet` communicates with the Kubernetes control plane via the `kube-apiserver`** ‚Äî the central and only access point to all cluster state and controller logic.

#### üîó Communication Flow:

* `kubelet` watches the **Pod specifications** assigned to its node via the `kube-apiserver`.
* It **reports node status**, **Pod status**, and **probe results** back to the API server.
* It doesn‚Äôt talk **directly** to the scheduler or controllers ‚Äî **only through the API server**.

---

### üß† Why do we say "controller"?

In Kubernetes, **controllers** are control loops that **watch resources** via the API server and **reconcile** the actual state with the desired state.

#### ü§ñ Common controllers (running inside `kube-controller-manager`):

* **ReplicaSet Controller**: Ensures correct number of Pod replicas.
* **Node Controller**: Monitors node health.
* **Job Controller**: Handles completion of batch jobs.
* **Deployment Controller**: Manages rollouts and updates.

So when we say "controller," we refer to any of these logic components that act upon the state reported to the `kube-apiserver`.

---

### 7. **How Does Kubernetes Process a Request?**

üì• Step-by-Step:

1. **Client Sends Request**

   * Could be an API call, HTTP request, etc.

2. **Ingress Point (depends on Service type)**

   * **`LoadBalancer`**: If exposed via a cloud LB, traffic hits the external LB ‚Üí forwarded to a Node.
   * **`Ingress`**: For HTTP/S traffic, goes through an Ingress Controller (like NGINX, Traefik).
   * **`NodePort`**: Accesses a high port on any node ‚Üí forwarded internally.
   * **`ExternalName`**: Simply resolves to an external DNS name (no traffic routing in cluster).

3. **kube-proxy (on the target node)**

   * Receives traffic for the **ClusterIP** of the Service.
   * Uses **iptables**, **IPVS**, or **eBPF** to route traffic to a healthy backend Pod.

4. **ClusterIP Service**

   * The virtual IP assigned to the Service.
   * kube-proxy balances traffic across one of the selected Pods.

5. **Pod**

   * Finally, the request reaches the container inside a Pod via the `targetPort`.

---

### 8. **How Does Kubernetes Handle Authentication and Authorization?**

Kubernetes separates **auth** into two key phases:

---

## üîê 1. **Authentication (Who are you?)**

Kubernetes first checks *who* is making the request. It supports several **authentication methods**, such as:

| Method             | Example Use Case                           |
| ------------------ | ------------------------------------------ |
| TLS Client Certs   | Admin users / kubelets                     |
| Bearer Tokens      | Service accounts / static tokens           |
| OIDC               | Login via Google, Azure AD, Keycloak, etc. |
| Webhook Token Auth | Custom external auth service               |
| Anonymous Access   | Disabled by default (for security reasons) |

The **`kube-apiserver`** performs this check.

---

## ‚úÖ 2. **Authorization (Are you allowed to do this?)**

Once the user is authenticated, Kubernetes checks whether they‚Äôre **authorized** to perform the requested action.

This is handled using:

### üîë **RBAC (Role-Based Access Control)**

RBAC uses:

* **Roles / ClusterRoles**: Define permissions (verbs like get, list, delete on resources).
* **RoleBindings / ClusterRoleBindings**: Attach those roles to users or service accounts.

---

## üìç Optional: Admission Control

After passing authentication and authorization, **admission controllers** can perform **validations, mutations, or policy enforcement** (e.g., deny creating a privileged Pod).

---

### 9. **How Does Communication Happen in Kubernetes?**

Kubernetes networking enables **Pod-to-Pod**, **Pod-to-Service**, and **intra-/inter-node** communication through a combination of:

---

## üîß Key Components of K8s Networking

| Component           | Role in Communication                                  |
| ------------------- | ------------------------------------------------------ |
| **CNI Plugin**      | Manages Pod IPs and Pod-to-Pod communication           |
| **kube-proxy**      | Routes Service traffic to Pods using ClusterIP         |
| **ClusterIP**       | Virtual IP for Services that routes traffic internally |
| **Node networking** | Ensures traffic can move between nodes                 |

---

### üì° 1. **Pod-to-Pod Communication (CNI)**

* Kubernetes expects **all Pods to have unique IPs** in a flat network.
* This is handled by the **CNI (Container Network Interface)** plugin.

  * Examples: Calico, Flannel, Cilium, Weave.
* The CNI ensures that:

  * Every Pod gets a routable IP.
  * Any Pod can talk to any other Pod ‚Äî on the **same or different nodes**.

> ‚úÖ No NAT is required for Pod-to-Pod traffic (in ideal CNI setups).

---

### üîÄ 2. **Pod-to-Service Communication (kube-proxy + ClusterIP)**

* Services (like `ClusterIP`) use **virtual IPs**.
* `kube-proxy` intercepts traffic to that IP and routes it to a healthy backend Pod using:

  * `iptables`
  * `IPVS`
  * `eBPF` (in advanced setups like Cilium)

> üìå The request is load-balanced across Pods selected by the Service‚Äôs label selector.

---

### üåê 3. **Cross-Node Communication**

* When Pods on different nodes talk, the traffic flows over the node's network.
* The CNI must support **inter-node routing**.
* kube-proxy ensures the request lands on the correct node and Pod.

---

## 10. **How Does the Kubernetes Network Work?**

---

## üß± Key Design Principles

Kubernetes has **two fundamental assumptions** about networking:

1. **Every Pod gets its own unique IP address.**
2. **All Pods can communicate with each other, without NAT.**

> This is very different from traditional networking, where multiple containers share one IP and use port mapping.

---

## üì¶ Networking Layers in Kubernetes

Let‚Äôs break it into the main components that make it work:

---

### 1. üß© **Container Network Interface (CNI)**

> The **CNI plugin** is responsible for assigning IP addresses to Pods and configuring network routes.

* When a Pod is created, Kubernetes calls the CNI to:

  * Assign a **unique IP** to the Pod
  * Set up **virtual networking** (bridge, veth pair)
  * Configure routing tables and interfaces
* Examples: **Calico**, **Flannel**, **Cilium**, **Weave**

üìå **CNI enables Pod-to-Pod communication**, even across nodes.

---

### 2. üîÑ **Service Networking (ClusterIP + kube-proxy)**

Kubernetes Services provide a **stable virtual IP** (ClusterIP) to access a group of Pods.

* `kube-proxy` runs on every node and handles traffic to ClusterIP Services.
* It uses:

  * `iptables` (rules in kernel)
  * `IPVS` (IP Virtual Server in kernel)
  * `eBPF` (modern method in Cilium)

üìå kube-proxy ensures **load balancing** and **routing** from Service IP ‚Üí healthy Pod IPs.

---

### 3. üö™ **Node Networking (Inter-node Traffic)**

* If Pods on different nodes need to talk, traffic goes over the **underlying node network**.
* The **CNI must handle inter-node routing**.

  * Example: Calico uses BGP to advertise Pod routes.
* Without this, Pods on different nodes wouldn‚Äôt be able to reach each other.

---

### 4. üåç **External Communication (Egress/Ingress)**

* **Ingress**: HTTP(S) traffic from outside ‚Üí exposed using **Ingress controllers** (NGINX, Traefik).
* **NodePort / LoadBalancer**: Allow external access to Services.
* **Egress**: Pods accessing the internet.

  * Usually NATed via the **node‚Äôs IP**.
  * Some CNI plugins (e.g., Calico) can control egress traffic too.

---

## üîÅ Common Traffic Flows

| Scenario                   | Flow                                                 |
| -------------------------- | ---------------------------------------------------- |
| Pod ‚Üí Pod (same node)      | CNI routes traffic directly                          |
| Pod ‚Üí Pod (different node) | CNI + underlying node network                        |
| Pod ‚Üí Service              | kube-proxy routes to a Pod via ClusterIP             |
| External ‚Üí Service         | LoadBalancer / NodePort / Ingress ‚Üí kube-proxy ‚Üí Pod |
| Pod ‚Üí Internet             | Pod IP SNAT‚Äôd to Node IP ‚Üí internet                  |

---

## 11. **Calico Vs Cilium**

---

## ü•ä Calico vs. Cilium

| Feature / Aspect             | **Calico**                                 | **Cilium**                                                      |
| ---------------------------- | ------------------------------------------ | --------------------------------------------------------------- |
| **Technology Base**          | Linux kernel routing + iptables / IPVS     | **eBPF** (kernel-level, faster + modern)                        |
| **Networking Model**         | Traditional L3 routing                     | Native L3/4/7 with eBPF                                         |
| **CNI Support**              | Yes                                        | Yes                                                             |
| **eBPF-based**               | Optional (Calico eBPF mode exists)         | ‚úÖ Built entirely on eBPF                                        |
| **Service Routing**          | kube-proxy or eBPF mode                    | **Replaces kube-proxy with eBPF**                               |
| **Network Policies**         | L3/L4 (IP, port-based)                     | ‚úÖ L3/L4 **+ L7-aware policies** (e.g., HTTP)                    |
| **Performance**              | Good (depends on mode)                     | **Excellent** (less context switch, no iptables)                |
| **Observability**            | Basic tools (Flow logs, Felix metrics)     | ‚úÖ **Hubble**: full observability with service maps, flows, etc. |
| **Encryption**               | WireGuard or IPsec                         | Native with eBPF + XDP                                          |
| **Kube-Proxy Replacement**   | Optional (only in eBPF mode)               | ‚úÖ Fully replaces kube-proxy                                     |
| **Cloud Native Integration** | AWS, GCP, Azure, OpenStack                 | AWS, GCP, Azure, etc.                                           |
| **Complexity**               | Mature and familiar (iptables-based)       | More modern, can be complex to troubleshoot                     |
| **Enterprise Features**      | Available via **Tigera Calico Enterprise** | Available via **Cilium Enterprise (Isovalent)**                 |

---

